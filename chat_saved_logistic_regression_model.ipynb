{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e49bb-6642-485b-b6db-3b04a5d54e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat_saved_logistic_regressionmodel"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8082b57-f72a-41f2-8b13-03f438404682",
   "metadata": {},
   "source": [
    "!pip install fastapi scikit-learn uvicorn pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca87bbfc-c0d5-456d-870c-6de3698e4ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    \"income\": [50000, 30000, 45000, 60000, 70000],\n",
    "    \"debt_to_income_ratio\": [0.2, 0.5, 0.4, 0.1, 0.25],\n",
    "    \"credit_score\": [700, 600, 650, 750, 720],\n",
    "    \"loan_amount\": [20000, 15000, 25000, 10000, 20000],\n",
    "    \"default\": [0, 1, 0, 0, 1]  # 0 = Non-default, 1 = Default\n",
    "})\n",
    "\n",
    "# FastAPI App\n",
    "app = FastAPI()\n",
    "\n",
    "# In-memory Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "X = data.drop(\"default\", axis=1)\n",
    "y = data[\"default\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Define the input schema for retraining\n",
    "class RetrainRequest(BaseModel):\n",
    "    penalty: str = \"l2\"\n",
    "    solver: str = \"lbfgs\"\n",
    "    max_iter: int = 100\n",
    "    test_size: float = 0.2\n",
    "\n",
    "@app.post(\"/retrain/\")\n",
    "def retrain(request: RetrainRequest):\n",
    "    global model, X_train, X_test, y_train, y_test\n",
    "    try:\n",
    "        # Retrain model with new parameters\n",
    "        model = LogisticRegression(\n",
    "            penalty=request.penalty,\n",
    "            solver=request.solver,\n",
    "            max_iter=request.max_iter\n",
    "        )\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=request.test_size, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "        test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "        return {\n",
    "            \"message\": \"Model retrained successfully\",\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"test_accuracy\": test_accuracy\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "def predict(features: dict):\n",
    "    try:\n",
    "        feature_array = np.array([list(features.values())]).reshape(1, -1)\n",
    "        prediction = model.predict(feature_array)[0]\n",
    "        probability = model.predict_proba(feature_array)[0, 1]\n",
    "        return {\n",
    "            \"prediction\": int(prediction),\n",
    "            \"probability\": float(probability)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e9ba49-bda7-43ba-898e-1513caa850da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  \n"
     ]
    },
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 52\u001b[0m\n\u001b[1;32m     45\u001b[0m             completion \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     46\u001b[0m                 model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-davinci-003\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     47\u001b[0m                 prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe user said: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. How should I respond?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m                 max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m\n\u001b[1;32m     49\u001b[0m             )\n\u001b[1;32m     50\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT:\u001b[39m\u001b[38;5;124m\"\u001b[39m, completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m---> 52\u001b[0m chat_with_user()\n",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m, in \u001b[0;36mchat_with_user\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Generic GPT response\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     completion \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     46\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-davinci-003\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     47\u001b[0m         prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe user said: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. How should I respond?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT:\u001b[39m\u001b[38;5;124m\"\u001b[39m, completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import requests\n",
    "\n",
    "# Set up OpenAI API\n",
    "openai.api_key = \"your-openai-api-key\"\n",
    "\n",
    "# Backend URLs\n",
    "BACKEND_URL = \"http://127.0.0.1:8000\"\n",
    "\n",
    "def chat_with_user():\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        if \"retrain\" in user_input.lower():\n",
    "            # Parse user input for retrain parameters\n",
    "            params = {\n",
    "                \"penalty\": \"l2\",\n",
    "                \"solver\": \"lbfgs\",\n",
    "                \"max_iter\": 200,\n",
    "                \"test_size\": 0.3\n",
    "            }  # Example: Adjust based on user input\n",
    "\n",
    "            # Call backend retrain API\n",
    "            response = requests.post(f\"{BACKEND_URL}/retrain/\", json=params)\n",
    "            print(\"GPT:\", response.json())\n",
    "\n",
    "        elif \"predict\" in user_input.lower():\n",
    "            # Example: Parse user features\n",
    "            features = {\n",
    "                \"income\": 55000,\n",
    "                \"debt_to_income_ratio\": 0.3,\n",
    "                \"credit_score\": 680,\n",
    "                \"loan_amount\": 15000\n",
    "            }  # Adjust based on user input\n",
    "\n",
    "            # Call backend predict API\n",
    "            response = requests.post(f\"{BACKEND_URL}/predict/\", json=features)\n",
    "            print(\"GPT:\", response.json())\n",
    "\n",
    "        else:\n",
    "            # Generic GPT response\n",
    "            completion = openai.Completion.create(\n",
    "                model=\"text-davinci-003\",\n",
    "                prompt=f\"The user said: {user_input}. How should I respond?\",\n",
    "                max_tokens=150\n",
    "            )\n",
    "            print(\"GPT:\", completion.choices[0].text.strip())\n",
    "\n",
    "chat_with_user()\n",
    "can you retain odel\n",
    "If I give "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9f048-9406-4226-9d1a-e12d976711bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e0dd39-35aa-48e6-858c-aef792024b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

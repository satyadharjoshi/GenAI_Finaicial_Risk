{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fb0ad69-0683-4532-a760-652091560544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-Tuning BERT with a Logistic Regression Layer\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc1ccfec-4b94-4a94-b6d9-caa156814d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91be2fed-3710-4c43-a896-ad1dfcccd607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b761887-e18b-4964-addb-381f577c5ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.61\n",
      "Test Accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Balance the dataset to include both classes\n",
    "def balance_classes(dataset, num_samples):\n",
    "    class_0 = [sample for sample in dataset if sample[\"label\"] == 0][:num_samples // 2]\n",
    "    class_1 = [sample for sample in dataset if sample[\"label\"] == 1][:num_samples // 2]\n",
    "    return class_0 + class_1\n",
    "\n",
    "balanced_train_data = balance_classes(tokenized_datasets[\"train\"], 100)\n",
    "balanced_eval_data = balance_classes(tokenized_datasets[\"test\"], 50)\n",
    "\n",
    "# Define the training arguments with optimizations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                 # Output directory\n",
    "    evaluation_strategy=\"no\",               # Disable evaluation during training\n",
    "    learning_rate=2e-5,                     # Learning rate\n",
    "    per_device_train_batch_size=4,          # Smaller batch size for faster training\n",
    "    per_device_eval_batch_size=4,           # Smaller batch size for evaluation\n",
    "    num_train_epochs=1,                     # Reduce number of epochs to 1\n",
    "    fp16=True,                              # Enable mixed precision training\n",
    "    max_steps=100,                          # Limit steps for faster execution\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Function to extract embeddings (logits) from BERT\n",
    "def extract_embeddings(model, dataset):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for sample in dataset:  # Loop through each item in the dataset\n",
    "            input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0)  # Add batch dimension\n",
    "            attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits.cpu().numpy()\n",
    "            embeddings.append(logits.flatten())  # Append flattened logits\n",
    "            labels.append(sample[\"label\"])  # Append label\n",
    "    return np.array(embeddings), np.array(labels)\n",
    "\n",
    "# Extract embeddings from the train and test datasets\n",
    "train_embeddings, train_labels = extract_embeddings(model, balanced_train_data)\n",
    "test_embeddings, test_labels = extract_embeddings(model, balanced_eval_data)\n",
    "\n",
    "# Train a logistic regression model on top of the embeddings\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(train_embeddings, train_labels)\n",
    "\n",
    "# Predict and evaluate\n",
    "train_preds = log_reg.predict(train_embeddings)\n",
    "test_preds = log_reg.predict(test_embeddings)\n",
    "\n",
    "train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e15f3295-d6a9-4ec9-8731-338d774a2726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 08:59, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./trained_model/tokenizer_config.json',\n",
       " './trained_model/special_tokens_map.json',\n",
       " './trained_model/vocab.txt',\n",
       " './trained_model/added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Trainer for BERT\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=balanced_train_data,\n",
    "    eval_dataset=balanced_eval_data,\n",
    ")\n",
    "\n",
    "# Train the model (BERT fine-tuning)\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Save the trained BERT model and tokenizer for future use\n",
    "model.save_pretrained(\"./trained_model\")\n",
    "tokenizer.save_pretrained(\"./trained_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e7f066e-88dd-4863-968f-e31acface89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before Logistic Regression: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate model accuracy\n",
    "def evaluate_model(model, dataset):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for sample in dataset:\n",
    "            input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0)  # Add batch dimension\n",
    "            attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0)  # Add batch dimension\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            pred = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            \n",
    "            predictions.extend(pred)  # Append the prediction directly\n",
    "            labels.append(sample[\"label\"])  # Append the label directly (no need to iterate)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate on the test set\n",
    "initial_accuracy = evaluate_model(model, balanced_eval_data)\n",
    "print(f\"Accuracy before Logistic Regression: {initial_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "704b422c-5452-40e0-a92b-511780ddee12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the fine-tuned model on the test set\n",
    "trainer.evaluate()\n",
    "\n",
    "# Predict using the fine-tuned BERT model\n",
    "def predict(texts, model, tokenizer):\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions.numpy()\n",
    "\n",
    "# Example prediction\n",
    "texts = [\"I love this movie!\", \"This movie was terrible.\"]\n",
    "predictions = predict(texts, model, tokenizer)\n",
    "print(\"Predictions:\", predictions)  # Output will be the predicted labels (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c49ae33-3b49-4739-9b58-d68a72a2c302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x706a00b17a10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ffb33bc-807d-4b45-8417-d2439580a450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bef8773e-e0bc-43fd-ae32-2331acd743f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love this movie!', 'This movie was terrible.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d1aa51a-892e-4ac1-bf87-6d07d1631281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5184704f-2631-48f2-a279-eedfe2fc5ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8fbc0cb-a5b1-43d4-9569-feda8f660bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0]\n"
     ]
    }
   ],
   "source": [
    "# Predict using the fine-tuned BERT model\n",
    "def predict(texts, model, tokenizer):\n",
    "    # Encode the input texts using the tokenizer\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "    \n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model to get logits\n",
    "        outputs = model(**encodings)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Convert logits to probabilities using softmax (optional but useful for multi-class)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get the predicted class (index of the highest probability)\n",
    "        predictions = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    return predictions.numpy()\n",
    "\n",
    "# Example prediction\n",
    "texts = [\"I love this movie!\", \"This movie was terrible.\"]\n",
    "predictions = predict(texts, model, tokenizer)\n",
    "print(\"Predictions:\", predictions)  # Output should show the predicted labels (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b0cdbe1-823b-437f-b795-6d7f487bf880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "texts = [\n",
    "    \"I love this movie! It's amazing and so entertaining.\",\n",
    "    \"This movie was terrible. I hated every minute of it.\",\n",
    "    \"The plot was great, but the acting could have been better.\",\n",
    "    \"A fantastic film with breathtaking visuals and a captivating story!\",\n",
    "    \"I was really bored throughout this film. It's not worth watching.\",\n",
    "    \"One of the best movies Iâ€™ve seen this year. I highly recommend it!\",\n",
    "    \"The movie started off strong but ended poorly.\",\n",
    "    \"Not bad, but not great either. Just an average movie.\",\n",
    "    \"I couldn't stop laughing at the jokes. This was a really funny film.\",\n",
    "    \"It was too predictable and lacked depth. Would not watch again.\",\n",
    "    \"What a disappointment! The trailer was way better than the actual movie.\",\n",
    "    \"Iâ€™m glad I watched it, but I wouldnâ€™t watch it again.\",\n",
    "    \"Incredible! A masterpiece that will be remembered for years.\",\n",
    "    \"This movie is a must-see for any fan of action films.\",\n",
    "    \"Absolutely awful. I donâ€™t understand the hype around it.\",\n",
    "    \"The soundtrack was amazing, and the acting was top-notch.\",\n",
    "    \"This was a complete waste of time. I would not recommend it to anyone.\",\n",
    "    \"The movie was too long and dragged on. Could have been much shorter.\",\n",
    "    \"A rollercoaster of emotions. I loved every second of it.\",\n",
    "    \"Itâ€™s a fun movie to watch with friends, but not the best out there.\",\n",
    "    \"A very emotional and touching story that left me in tears.\",\n",
    "    \"The special effects were spectacular, but the story was lacking.\",\n",
    "    \"I didnâ€™t connect with the characters at all, but the film was well made.\",\n",
    "    \"If you're looking for something lighthearted, this movie is perfect.\",\n",
    "    \"A complete masterpiece from start to finish. I loved everything about it!\",\n",
    "    \"This movie is definitely overrated. I donâ€™t see what the fuss is about.\"\n",
    "]\n",
    "\n",
    "# Predictions\n",
    "predictions = predict(texts, model, tokenizer)\n",
    "print(\"Predictions:\", predictions)  # Output will be the predicted labels (0 or 1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "808f7cb7-6a13-4dd5-8992-2bc476138811",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load a larger subset of the dataset (20% for both train and test)\n",
    "dataset = load_dataset(\"imdb\", split='train[:20%]')\n",
    "test_dataset = load_dataset(\"imdb\", split='test[:20%]')\n",
    "\n",
    "# 2. Load the tokenizer and model (DistilBERT)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 3. Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_data = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 4. Custom balance function (if necessary) - random oversampling\n",
    "def balance_classes(dataset, num_samples):\n",
    "    # Separate by class labels\n",
    "    class_0 = [sample for sample in dataset if sample[\"label\"] == 0]\n",
    "    class_1 = [sample for sample in dataset if sample[\"label\"] == 1]\n",
    "    \n",
    "    # Check if both classes exist\n",
    "    if len(class_0) == 0 or len(class_1) == 0:\n",
    "        print(\"One of the classes is missing. Class 0 samples:\", len(class_0), \", Class 1 samples:\", len(class_1))\n",
    "        return dataset  # Return the dataset as is if one class is missing\n",
    "    \n",
    "    # Random oversample class 1 to balance the classes\n",
    "    if len(class_1) < len(class_0):\n",
    "        class_1 = class_1 * (len(class_0) // len(class_1)) + class_1[:len(class_0) % len(class_1)]\n",
    "    \n",
    "    balanced_data = class_0 + class_1  # Combine both classes\n",
    "    shuffle(balanced_data)  # Shuffle the dataset to ensure randomness\n",
    "    \n",
    "    # Check class distribution after balancing\n",
    "    class_0_count = sum([1 for sample in balanced_data if sample[\"label\"] == 0])\n",
    "    class_1_count = sum([1 for sample in balanced_data if sample[\"label\"] == 1])\n",
    "    print(f\"Class 0 samples: {class_0_count}, Class 1 samples: {class_1_count}\")\n",
    "    \n",
    "    return balanced_data\n",
    "\n",
    "# Apply custom balancing\n",
    "balanced_train_data = balance_classes(tokenized_datasets, 100)\n",
    "balanced_test_data = balance_classes(tokenized_test_data, 50)\n",
    "\n",
    "# 5. Extract embeddings (features) from DistilBERT\n",
    "def extract_embeddings(model, dataset):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for sample in dataset:\n",
    "            input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0)  # Add batch dimension\n",
    "            attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits.cpu().numpy()\n",
    "            embeddings.append(logits.flatten())  # Append flattened logits\n",
    "            labels.append(sample[\"label\"])  # Append label\n",
    "    return np.array(embeddings), np.array(labels)\n",
    "\n",
    "# Extract embeddings from the balanced train and test datasets\n",
    "train_embeddings, train_labels = extract_embeddings(model, balanced_train_data)\n",
    "test_embeddings, test_labels = extract_embeddings(model, balanced_test_data)\n",
    "\n",
    "# 6. Ensure both classes are present before applying feature selection\n",
    "unique_train_labels = np.unique(train_labels)\n",
    "print(f\"Classes in the training data: {unique_train_labels}\")  # Check which classes are present\n",
    "if len(unique_train_labels) < 2:\n",
    "    raise ValueError(\"The training data contains samples from only one class.\")\n",
    "\n",
    "# 7. Apply feature selection (SelectKBest)\n",
    "# Select top 50 features for faster processing\n",
    "selector = SelectKBest(score_func=f_classif, k=50)\n",
    "train_embeddings_selected = selector.fit_transform(train_embeddings, train_labels)\n",
    "test_embeddings_selected = selector.transform(test_embeddings)\n",
    "\n",
    "# 8. Identify the selected and removed features\n",
    "selected_features = selector.get_support(indices=True)  # Indices of selected features\n",
    "removed_features = [i for i in range(train_embeddings.shape[1]) if i not in selected_features]\n",
    "\n",
    "# Display the selected and removed features\n",
    "print(\"Selected feature indices:\", selected_features)\n",
    "print(\"Removed feature indices:\", removed_features)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb948b83-7d7c-4997-9633-965a1071ca56",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the full dataset without reducing the size\n",
    "dataset = load_dataset(\"imdb\", split='train[:10%]')  # Use a small portion (10%) to avoid too large data\n",
    "test_dataset = load_dataset(\"imdb\", split='test[:10%]')  # Use 10% of the test data\n",
    "\n",
    "# 2. Load the tokenizer and model (DistilBERT)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 3. Tokenize the dataset with a smaller max_length\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=32)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_data = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 4. Custom balance function (if necessary) - random oversampling or undersampling\n",
    "def balance_classes(dataset, num_samples):\n",
    "    # Separate by class labels\n",
    "    class_0 = [sample for sample in dataset if sample[\"label\"] == 0]\n",
    "    class_1 = [sample for sample in dataset if sample[\"label\"] == 1]\n",
    "    \n",
    "    # If one class is missing, handle it by adding a small sample from the available class\n",
    "    if len(class_0) == 0 or len(class_1) == 0:\n",
    "        print(\"One class is missing. Using available class only.\")\n",
    "        return dataset  # Return the dataset as is if one class is missing\n",
    "    \n",
    "    # Ensure balanced classes by resampling the smaller class\n",
    "    if len(class_0) < len(class_1):\n",
    "        class_0 = class_0 * (len(class_1) // len(class_0)) + class_0[:len(class_1) % len(class_0)]\n",
    "    elif len(class_1) < len(class_0):\n",
    "        class_1 = class_1 * (len(class_0) // len(class_1)) + class_1[:len(class_0) % len(class_1)]\n",
    "    \n",
    "    balanced_data = class_0 + class_1  # Combine both classes\n",
    "    shuffle(balanced_data)  # Shuffle the dataset to ensure randomness\n",
    "    \n",
    "    # Check class distribution after balancing\n",
    "    class_0_count = sum([1 for sample in balanced_data if sample[\"label\"] == 0])\n",
    "    class_1_count = sum([1 for sample in balanced_data if sample[\"label\"] == 1])\n",
    "    print(f\"Class 0 samples: {class_0_count}, Class 1 samples: {class_1_count}\")\n",
    "    \n",
    "    return balanced_data\n",
    "\n",
    "# Apply custom balancing\n",
    "balanced_train_data = balance_classes(tokenized_datasets, 1000)\n",
    "balanced_test_data = balance_classes(tokenized_test_data, 500)\n",
    "\n",
    "# 5. Extract embeddings (features) from DistilBERT\n",
    "def extract_embeddings(model, dataset):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for sample in dataset:\n",
    "            input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0)  # Add batch dimension\n",
    "            attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits.cpu().numpy()\n",
    "            embeddings.append(logits.flatten())  # Append flattened logits\n",
    "            labels.append(sample[\"label\"])  # Append label\n",
    "    return np.array(embeddings), np.array(labels)\n",
    "\n",
    "# Extract embeddings from the balanced train and test datasets\n",
    "train_embeddings, train_labels = extract_embeddings(model, balanced_train_data)\n",
    "test_embeddings, test_labels = extract_embeddings(model, balanced_test_data)\n",
    "\n",
    "# 6. Display the class distribution\n",
    "unique_train_labels = np.unique(train_labels)\n",
    "print(f\"Classes in the training data: {unique_train_labels}\")\n",
    "\n",
    "# 7. Final output with reduced data\n",
    "print(f\"Train embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"Test embeddings shape: {test_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e190942d-43e4-42b7-81cd-4a7ad67b5bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One class is missing. Using available class only.\n",
      "One class is missing. Using available class only.\n",
      "Total time for extracting embeddings: 144.7922 seconds.\n",
      "Total time for extracting embeddings: 157.4270 seconds.\n",
      "Classes in the training data: [0]\n",
      "Train embeddings shape: (2500, 2)\n",
      "Test embeddings shape: (2500, 2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import time  # Import time module to track duration\n",
    "\n",
    "# 1. Load the full dataset without reducing the size\n",
    "dataset = load_dataset(\"imdb\", split='train[:10%]')  # Use a small portion (10%) to avoid too large data\n",
    "test_dataset = load_dataset(\"imdb\", split='test[:10%]')  # Use 10% of the test data\n",
    "\n",
    "# 2. Load the tokenizer and model (DistilBERT)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 3. Tokenize the dataset with a smaller max_length\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncatValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0ion=True, max_length=32)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_data = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 4. Custom balance function (if necessary) - random oversampling or undersampling\n",
    "def balance_classes(dataset, num_samples):\n",
    "    # Separate by class labels\n",
    "    class_0 = [sample for sample in dataset if sample[\"label\"] == 0]\n",
    "    class_1 = [sample for sample in dataset if sample[\"label\"] == 1]\n",
    "    \n",
    "    # If one class is missing, handle it by adding a small sample from the available class\n",
    "    if len(class_0) == 0 or len(class_1) == 0:\n",
    "        print(\"One class is missing. Using available class only.\")\n",
    "        return dataset  # Return the dataset as is if one class is missing\n",
    "    \n",
    "    # Ensure balanced classes by resampling the smaller class\n",
    "    if len(class_0) < len(class_1):\n",
    "        class_0 = class_0 * (len(class_1) // len(class_0)) + class_0[:len(class_1) % len(class_0)]\n",
    "    elif len(class_1) < len(class_0):\n",
    "        class_1 = class_1 * (len(class_0) // len(class_1)) + class_1[:len(class_0) % len(class_1)]\n",
    "    \n",
    "    balanced_data = class_0 + class_1  # Combine both classes\n",
    "    shuffle(balanced_data)  # Shuffle the dataset to ensure randomness\n",
    "    \n",
    "    # Check class distribution after balancing\n",
    "    class_0_count = sum([1 for sample in balanced_data if sample[\"label\"] == 0])\n",
    "    class_1_count = sum([1 for sample in balanced_data if sample[\"label\"] == 1])\n",
    "    print(f\"Class 0 samples: {class_0_count}, Class 1 samples: {class_1_count}\")\n",
    "    \n",
    "    return balanced_data\n",
    "\n",
    "# Apply custom balancing\n",
    "balanced_train_data = balance_classes(tokenized_datasets, 1000)\n",
    "balanced_test_data = balance_classes(tokenized_test_data, 500)\n",
    "\n",
    "# 5. Extract embeddings (features) from DistilBERT with time tracking\n",
    "def extract_embeddings(model, dataset):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    start_time = time.time()  # Record the start time for the entire extraction process\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(dataset):\n",
    "            epoch_start_time = time.time()  # Record the time at the start of each step\n",
    "            \n",
    "            input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0)  # Add batch dimension\n",
    "            attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits.cpu().numpy()\n",
    "            embeddings.append(logits.flatten())  # Append flattened logits\n",
    "            labels.append(sample[\"label\"])  # Append label\n",
    "            \n",
    "            epoch_end_time = time.time()  # Record the time after each step\n",
    "    \n",
    "    total_end_time = time.time()  # Record the total time taken for all steps\n",
    "    print(f\"Total time for extracting embeddings: {total_end_time - start_time:.4f} seconds.\")\n",
    "    \n",
    "    return np.array(embeddings), np.array(labels)\n",
    "\n",
    "# Extract embeddings from the balanced train and test datasets\n",
    "train_embeddings, train_labels = extract_embeddings(model, balanced_train_data)\n",
    "test_embeddings, test_labels = extract_embeddings(model, balanced_test_data)\n",
    "\n",
    "# 6. Display the class distribution\n",
    "unique_train_labels = np.unique(train_labels)\n",
    "print(f\"Classes in the training data: {unique_train_labels}\")\n",
    "\n",
    "# 7. Final output with reduced data\n",
    "print(f\"Train embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"Test embeddings shape: {test_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90af5229-043f-4e63-aa12-88fd6223cdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before Logistic Regression: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate model accuracy\n",
    "def evaluate_model(model, dataset):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for sample in dataset:\n",
    "            input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0)  # Add batch dimension\n",
    "            attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0)  # Add batch dimension\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            pred = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            \n",
    "            predictions.extend(pred)  # Append the prediction directly\n",
    "            labels.append(sample[\"label\"])  # Append the label directly (no need to iterate)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate on the test set\n",
    "initial_accuracy = evaluate_model(model, balanced_eval_data)\n",
    "print(f\"Accuracy before Logistic Regression: {initial_accuracy}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09224858-c6cc-40f0-8062-08934f1ea63a",
   "metadata": {},
   "source": [
    "\n",
    "# 8. Train a Logistic Regression model on the selected features\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(train_embeddings, train_labels)\n",
    "\n",
    "# 9. Evaluate the model\n",
    "train_preds = log_reg.predict(train_embeddings)\n",
    "test_preds = log_reg.predict(test_embeddings)\n",
    "\n",
    "train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948bbb4-8942-4fae-8598-f1cb73079078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
